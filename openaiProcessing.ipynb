{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "api_key = \"api_key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Generating open ai files for batch processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(dfs):\n",
    "    comments = df[\"Texto Comentario\"].tolist()\n",
    "    \n",
    "    for prompt_idx, prompt in enumerate(prompts):\n",
    "        # Nombre del archivo de salida\n",
    "        input_filename = f'data/batches/prompt{prompt_idx + 1}/input_comments_batch_{idx + 1}.jsonl'\n",
    "        \n",
    "        # Crear directorio si no existe\n",
    "        os.makedirs(os.path.dirname(input_filename), exist_ok=True)\n",
    "        \n",
    "        # Abrir y escribir en el archivo JSONL con codificación UTF-8\n",
    "        with open(input_filename, 'w', encoding='utf-8') as jsonl_file:\n",
    "            for i, comment in enumerate(comments, start=1):\n",
    "                request_data = {\n",
    "                    \"custom_id\": f\"request-{i}\",\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": {\n",
    "                        \"model\": \"gpt-4o-mini\",\n",
    "                        \"temperature\": 0,\n",
    "                        \"messages\": [\n",
    "                            {\"role\": \"system\", \"content\": prompt},\n",
    "                            {\"role\": \"user\", \"content\": comment}\n",
    "                        ],\n",
    "                        \"max_tokens\": 1000\n",
    "                    }\n",
    "                }\n",
    "                jsonl_file.write(json.dumps(request_data, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"Archivo JSONL creado para el lote {idx + 1} y el prompt {prompt_idx + 1}.\")\n",
    "\n",
    "print(\"Archivos JSONL creados correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Response processing function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_respuestas(file_response):\n",
    "    # Leer el contenido del archivo como texto\n",
    "    file_content = file_response.text\n",
    "    \n",
    "    # Convertir cada línea del archivo JSONL en un diccionario\n",
    "    lines = file_content.splitlines()\n",
    "    responses = [json.loads(line) for line in lines]\n",
    "    \n",
    "    # Extraer la respuesta del asistente\n",
    "    assistant_responses = [\n",
    "        response['response']['body']['choices'][0]['message']['content'] \n",
    "        for response in responses\n",
    "    ]\n",
    "    \n",
    "    # Crear un DataFrame con las respuestas en una columna\n",
    "    df_respuestas = pd.DataFrame(assistant_responses, columns=['respuesta'])\n",
    "    \n",
    "    # Mostrar el DataFrame\n",
    "    return df_respuestas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Run batch per prompts an index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'prompts' es una lista de los textos de los prompts\n",
    "prompt = prompts[0]  # Selecciona el primer prompt\n",
    "prompt_idx = 5  # Índice del primer prompt\n",
    "\n",
    "# Establecer el índice de inicio, establecer a 0 para procesar todos desde el principio\n",
    "start_index = 5\n",
    "\n",
    "# Usar 'start_index' para definir el inicio del bucle\n",
    "for idx, df in enumerate(dfs[start_index:], start=start_index):\n",
    "\n",
    "    # Nombre del archivo de entrada basado en el primer prompt y el lote\n",
    "    input_filename = f'data/batches/prompt{prompt_idx + 1}/input_comments_batch_{idx + 1}.jsonl'\n",
    "    \n",
    "    # Subir el archivo a OpenAI\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(input_filename, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    # Crear un lote de procesamiento\n",
    "    batch_process = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\"description\": f\"prompt{prompt_idx + 1}.final\"}\n",
    "    )\n",
    "    batch_id = batch_process.id\n",
    "\n",
    "    print(f\"Lote {idx + 1} y prompt {prompt_idx + 1} creado.\")\n",
    "    print(f\"ID del lote: {batch_id}\")\n",
    "\n",
    "    # Esperar a que el lote se confirme que está en proceso\n",
    "    while True:\n",
    "        status = client.batches.retrieve(batch_id).status\n",
    "        if status == \"processing\":\n",
    "            print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} está en proceso.\")\n",
    "            break\n",
    "        elif status not in [\"queued\", \"processing\"]:\n",
    "            print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} ha fallado o sido cancelado en la etapa de inicio.\")\n",
    "            break\n",
    "        time.sleep(10)  # Espera 10 segundos antes de comprobar de nuevo\n",
    "\n",
    "    # Esperar a que el lote se complete\n",
    "    while True:\n",
    "        status = client.batches.retrieve(batch_id).status\n",
    "        if status == \"completed\":\n",
    "            # si el lote se ha completado, procesar las respuestas\n",
    "            file_response_t = client.batches.retrieve(batch_id).output_file_id  # se obtiene el id del archivo de salida\n",
    "            file_response = client.files.content(file_response_t)\n",
    "            df_respuestas = procesar_respuestas(file_response)  # Asegúrate de que esta función retorne un DataFrame\n",
    "            print(f\"Respuestas procesadas para el lote {idx + 1} y el prompt {prompt_idx + 1}.\")\n",
    "            df_respuestas.to_csv(f'data/batches/prompt{prompt_idx + 1}/results2/batch_{idx + 1}_with_responses.csv', index=False)\n",
    "            break\n",
    "        elif status in [\"failed\", \"cancelled\"]:\n",
    "            print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} ha fallado o sido cancelado.\")\n",
    "            break\n",
    "        time.sleep(100)  # Espera 100 segundos antes de comprobar de nuevo\n",
    "\n",
    "print(\"Procesamiento de respuestas completado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **All prompts processing fuction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponiendo que 'prompts' es una lista de los textos de los prompts\n",
    "start_index = 0  # Establecer el índice de inicio para los DataFrame\n",
    "\n",
    "# Iniciar iteración sobre los prompts desde el índice 1 (que corresponde al segundo prompt)\n",
    "for prompt_idx, prompt in enumerate(prompts[6:], start=6): \n",
    "    \n",
    "    # Iterar sobre cada DataFrame a partir del índice de inicio\n",
    "    for idx, df in enumerate(dfs[start_index:], start=start_index):\n",
    "        \n",
    "        # Nombre del archivo de entrada basado en el prompt y el lote\n",
    "        input_filename = f'data/batches/prompt{prompt_idx + 1}/input_comments_batch_{idx + 1}.jsonl'\n",
    "        \n",
    "        # Crear la carpeta 'results2' si no existe\n",
    "        results_dir = f'data/batches/prompt{prompt_idx + 1}/results2'\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        # Subir el archivo a OpenAI\n",
    "        batch_input_file = client.files.create(\n",
    "            file=open(input_filename, \"rb\"),\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "        batch_input_file_id = batch_input_file.id\n",
    "\n",
    "        # Crear un lote de procesamiento\n",
    "        batch_process = client.batches.create(\n",
    "            input_file_id=batch_input_file_id,\n",
    "            endpoint=\"/v1/chat/completions\",\n",
    "            completion_window=\"24h\",\n",
    "            metadata={\"description\": f\"prompt{prompt_idx + 1}.final\"}\n",
    "        )\n",
    "        batch_id = batch_process.id\n",
    "\n",
    "        print(f\"Lote {idx + 1} y prompt {prompt_idx + 1} creado.\")\n",
    "        print(f\"ID del lote: {batch_id}\")\n",
    "\n",
    "        # Esperar a que el lote se confirme que está en proceso\n",
    "        while True:\n",
    "            status = client.batches.retrieve(batch_id).status\n",
    "            if status == \"processing\":\n",
    "                print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} está en proceso.\")\n",
    "                break\n",
    "            elif status not in [\"queued\", \"processing\"]:\n",
    "                print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} ha fallado o sido cancelado en la etapa de inicio.\")\n",
    "                break\n",
    "            time.sleep(10)  # Espera 10 segundos antes de comprobar de nuevo\n",
    "\n",
    "        # Esperar a que el lote se complete\n",
    "        while True:\n",
    "            status = client.batches.retrieve(batch_id).status\n",
    "            if status == \"completed\":\n",
    "                # si el lote se ha completado, procesar las respuestas\n",
    "                file_response_t = client.batches.retrieve(batch_id).output_file_id  # se obtiene el id del archivo de salida\n",
    "                file_response = client.files.content(file_response_t)\n",
    "                df_respuestas = procesar_respuestas(file_response)  # Asegúrate de que esta función retorne un DataFrame\n",
    "                print(f\"Respuestas procesadas para el lote {idx + 1} y el prompt {prompt_idx + 1}.\")\n",
    "                \n",
    "                # Guardar las respuestas en la carpeta 'results2'\n",
    "                df_respuestas.to_csv(f'{results_dir}/batch_{idx + 1}_with_responses.csv', index=False)\n",
    "                break\n",
    "            elif status in [\"failed\", \"cancelled\"]:\n",
    "                print(f\"El lote {idx + 1} y el prompt {prompt_idx + 1} ha fallado o sido cancelado.\")\n",
    "                break\n",
    "            time.sleep(100)  # Espera 100 segundos antes de comprobar de nuevo\n",
    "\n",
    "print(\"Procesamiento de respuestas completado.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
